{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7539f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0434ad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import baccoemu\n",
    "import chainconsumer\n",
    "import dynesty\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "import emcee\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('/dipc/kstoreyf/muchisimocks/scripts')\n",
    "import sbi_tools\n",
    "import plot_utils\n",
    "#import scripts\n",
    "# from scripts import sbi_tools\n",
    "#from scripts import plot_utils\n",
    "import generate_emuPks as genP\n",
    "\n",
    "from momentnetworks import demo\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bfaf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_threads = 24\n",
    "tf.config.threading.set_inter_op_parallelism_threads(n_threads)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(n_threads)\n",
    "# note: does NOT work to call sbi_tools.set_N_threads for some reason, need to do it here\n",
    "torch.set_num_threads(n_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4bb938",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_moment = True\n",
    "run_sbi = False\n",
    "run_emcee = True\n",
    "run_dynesty = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb08c45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dir = '../plots/plots_2024-03-15'\n",
    "save_plots = True\n",
    "\n",
    "tag_fit = '_emuPk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10474737",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ncpu = cpu_count()\n",
    "print(\"{0} CPUs\".format(ncpu))\n",
    "print(f\"Using {n_threads} threads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750d4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "mpl.pyplot.style.use('default')\n",
    "mpl.pyplot.close('all')\n",
    "\n",
    "font, rcnew = plot_utils.matplotlib_default_config()\n",
    "mpl.rc('font', **font)\n",
    "mpl.pyplot.rcParams.update(rcnew)\n",
    "mpl.pyplot.style.use('tableau-colorblind10')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "mpl.rcParams['xtick.labelsize'] = 16 \n",
    "mpl.rcParams['ytick.labelsize'] = 16 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1d7912",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b738e593",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_emuPk = '_2param'\n",
    "#tag_errG = f'_boxsize500'\n",
    "tag_errG = f''\n",
    "n_tot = 2000\n",
    "\n",
    "fn_emuPk = f'../data/emuPks/emuPks{tag_emuPk}.npy'\n",
    "fn_emuPk_params = f'../data/emuPks/emuPks_params{tag_emuPk}.txt'\n",
    "fn_emuk = f'../data/emuPks/emuPks_k{tag_emuPk}.txt'\n",
    "fn_emuPkerrG = f'../data/emuPks/emuPks_errgaussian{tag_emuPk}{tag_errG}.npy'\n",
    "\n",
    "Pk_noiseless = np.load(fn_emuPk)\n",
    "gaussian_error_pk = np.load(fn_emuPkerrG)\n",
    "theta = np.genfromtxt(fn_emuPk_params, delimiter=',', names=True)\n",
    "param_names = theta.dtype.names\n",
    "# from tuples to 2d array\n",
    "theta = np.array([list(tup) for tup in theta])\n",
    "\n",
    "kk = np.genfromtxt(fn_emuk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232a4df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "idx_select = rng.choice(np.arange(Pk_noiseless.shape[0]), n_tot, replace=False)\n",
    "Pk_noiseless = Pk_noiseless[idx_select]\n",
    "gaussian_error_pk = gaussian_error_pk[idx_select]\n",
    "theta = theta[idx_select]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10591cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tot = theta.shape[0]\n",
    "n_params = theta.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f2bcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_label_dict = {'omega_cold': r'$\\Omega_\\mathrm{m}$',\n",
    "                'sigma8_cold': r'$\\sigma_{8}$',\n",
    "                'sigma_8': r'$\\sigma_{8}$',\n",
    "                'hubble': r'$h$',\n",
    "                'h': r'$h$',\n",
    "                'ns': r'$n_\\mathrm{s}$',\n",
    "                'n_s': r'$n_\\mathrm{s}$',\n",
    "                'omega_baryon': r'$\\Omega_\\mathrm{b}$',}\n",
    "param_labels = [param_label_dict[param_name] for param_name in param_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7897e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Pk_noiseless.shape, theta.shape, gaussian_error_pk.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b41c5dc",
   "metadata": {},
   "source": [
    "Add some error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc41182",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "Pk = rng.normal(Pk_noiseless, gaussian_error_pk)\n",
    "print(Pk.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c9fe6a",
   "metadata": {},
   "source": [
    "Plot P(k) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843917db",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plot = 100\n",
    "fig, ax = mpl.pyplot.subplots(figsize=(6, 4.5))\n",
    "for iLH in range(n_plot):\n",
    "    ax.loglog(kk, gaussian_error_pk[iLH])\n",
    "\n",
    "ax.set_xlabel(r'$k \\,\\, [h \\,\\, {\\rm Mpc}^{-1}]$', fontsize=23)\n",
    "ax.set_ylabel(r'$\\sigma_\\text{G}(k)$', fontsize=23)\n",
    "\n",
    "mpl.pyplot.tight_layout()\n",
    "mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6117cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = mpl.pyplot.subplots(figsize=(6, 5))\n",
    "for iLH in range(n_plot):\n",
    "    ax.loglog(kk, Pk_noiseless[iLH])\n",
    "\n",
    "ax.set_xlabel(r'$k \\,\\, [h \\,\\, {\\rm Mpc}^{-1}]$', fontsize=23)\n",
    "ax.set_ylabel(r'$P(k) \\,\\, [h^{-3} \\,\\, {\\rm Mpc}^3]$', fontsize=23)\n",
    "\n",
    "ax.set_xlim(1e-2, 0.75)\n",
    "ax.set_ylim(1e3, 3e5)\n",
    "mpl.pyplot.tight_layout()\n",
    "mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832741bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = mpl.pyplot.subplots(figsize=(6, 5))\n",
    "for iLH in range(n_plot):\n",
    "    ax.loglog(kk, gaussian_error_pk[iLH]/Pk_noiseless[iLH])\n",
    "\n",
    "ax.set_xlabel(r'$k \\,\\, [h \\,\\, {\\rm Mpc}^{-1}]$', fontsize=23)\n",
    "ax.set_ylabel(r'$\\sigma_\\text{G}(k)/P(k)$', fontsize=23)\n",
    "\n",
    "ax.set_xlim(1e-2, 0.75)\n",
    "#ax.set_ylim(1e3, 3e5)\n",
    "mpl.pyplot.tight_layout()\n",
    "mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52692bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = mpl.pyplot.subplots(figsize=(6, 5))\n",
    "for iLH in range(n_plot):\n",
    "    ax.loglog(kk, Pk[iLH])\n",
    "\n",
    "ax.set_xlabel(r'$k \\,\\, [h \\,\\, {\\rm Mpc}^{-1}]$', fontsize=23)\n",
    "ax.set_ylabel(r'$P(k) \\,\\, [h^{-3} \\,\\, {\\rm Mpc}^3]$', fontsize=23)\n",
    "\n",
    "ax.set_xlim(1e-2, 0.75)\n",
    "ax.set_ylim(1e3, 3e5)\n",
    "\n",
    "mpl.pyplot.tight_layout()\n",
    "mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770c775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(theta[:, 0], theta[:, 1], c='k', s=4, alpha=0.5)\n",
    "plt.xlabel(param_labels[0])\n",
    "plt.ylabel(param_labels[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a6629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_biasmodels = len(biases_vec)\n",
    "n_biasmodels = 0\n",
    "n_cosmos = n_params\n",
    "print(n_biasmodels, n_cosmos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f41673",
   "metadata": {},
   "source": [
    "Split into train-val-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fa276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p_train, p_test = 0.2, 0.75\n",
    "p_train, p_test = 0.4, 0.5\n",
    "p_val = 1-p_train-p_test\n",
    "train_split = int(theta.shape[0]*p_train)\n",
    "test_split = int(theta.shape[0]*(1-p_test))\n",
    "#train_val_split = int(n_biasmodels*round(theta.shape[0]*0.99/n_biasmodels))\n",
    "\n",
    "theta_train = theta[:train_split]\n",
    "theta_val = theta[train_split:test_split]\n",
    "theta_test = theta[test_split:]\n",
    "print(theta_train.shape, theta_val.shape, theta_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7a38ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pk_train = Pk[:train_split]\n",
    "Pk_val = Pk[train_split:test_split]\n",
    "Pk_test = Pk[test_split:]\n",
    "\n",
    "Pk_noiseless_train = Pk_noiseless[:train_split]\n",
    "Pk_noiseless_val = Pk_noiseless[train_split:test_split]\n",
    "Pk_noiseless_test = Pk_noiseless[test_split:]\n",
    "\n",
    "# should really do this mask just on training set, but for now have some bad test data for some reason\n",
    "mask = np.all(Pk>0, axis=0)\n",
    "Pk_train = Pk_train[:,mask]\n",
    "Pk_val = Pk_val[:,mask]\n",
    "Pk_test = Pk_test[:,mask]\n",
    "Pk_noiseless_train = Pk_noiseless_train[:,mask]\n",
    "Pk_noiseless_val = Pk_noiseless_val[:,mask]\n",
    "Pk_noiseless_test = Pk_noiseless_test[:,mask]\n",
    "k = kk[mask]\n",
    "\n",
    "gaussian_error_pk_train = gaussian_error_pk[:train_split][:,mask]\n",
    "gaussian_error_pk_val = gaussian_error_pk[train_split:test_split][:,mask]\n",
    "gaussian_error_pk_test = gaussian_error_pk[test_split:][:,mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f3f1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = Pk_train.shape[1]\n",
    "print(n_tot, n_params, n_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261b32cd",
   "metadata": {},
   "source": [
    "Set up test model now so we can viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41615173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = mpl.pyplot.subplots(1,1, figsize=(7,5))\n",
    "# fontsize = 24\n",
    "# fontsize1 = 18\n",
    "\n",
    "# alpha = 0.7\n",
    "\n",
    "# tmp_Pk_plot = Pk_train\n",
    "# tmp_Pk_plot = tmp_Pk_plot[np.random.choice(tmp_Pk_plot.shape[0], tmp_Pk_plot.shape[0], replace=False)].T\n",
    "# ax.plot(np.log10(k), np.log10(tmp_Pk_plot), c='royalblue', alpha=alpha, lw=0.5, label='training set')\n",
    "\n",
    "# tmp_Pk_plot = Pk_test\n",
    "# tmp_Pk_plot = tmp_Pk_plot[np.random.choice(tmp_Pk_plot.shape[0], tmp_Pk_plot.shape[0], replace=False)].T\n",
    "# ax.plot(np.log10(k), np.log10(tmp_Pk_plot), c='k', alpha=alpha, lw=0.5, label='test set')\n",
    "    \n",
    "# ax.plot(np.log10(k), np.log10(Pk_test[idx_test]), c='m', alpha=alpha, lw=2, label='test data')\n",
    "    \n",
    "# ax.set_xlabel(r'$k \\,\\, [h \\,\\, {\\rm Mpc}^{-1}]$', fontsize=23)\n",
    "# ax.set_ylabel(r'$P(k) \\,\\, [h^{-3} \\,\\, {\\rm Mpc}^3]$', fontsize=23)\n",
    "\n",
    "# mpl.pyplot.tight_layout()\n",
    "# mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b493f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler:\n",
    "\n",
    "    def __init__(self, func='log_minmax'):\n",
    "        self.func = func\n",
    "        \n",
    "    def fit(self, x_train):\n",
    "        self.x_train_min = np.min(x_train, axis=0)\n",
    "        self.x_train_max = np.max(x_train, axis=0)\n",
    "\n",
    "    def scale(self, x):\n",
    "        if self.func=='log_minmax':\n",
    "            return self.scale_log_minmax(x)\n",
    "        elif self.func=='minmax':\n",
    "            return self.scale_minmax(x)\n",
    "        else:\n",
    "            raise ValueError(f\"Function {self.func} not recognized\")\n",
    "\n",
    "    def unscale(self, x):\n",
    "        if self.func=='log_minmax':\n",
    "            return self.unscale_log_minmax(x)\n",
    "        elif self.func=='minmax':\n",
    "            return self.unscale_minmax(x)\n",
    "        else:\n",
    "            raise ValueError(f\"Function {self.func} not recognized\")\n",
    "       \n",
    "    def scale_error(self, err, x):\n",
    "        if self.func=='log_minmax':\n",
    "            return self.scale_error_log_minmax(err, x)\n",
    "        elif self.func=='minmax':\n",
    "            raise ValueError(f\"Error not implemented for {self.func}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Function {self.func} not recognized\")\n",
    "\n",
    "    def scale_log_minmax(self, x):\n",
    "        log_x = np.log10(x)\n",
    "        log_x_norm = (log_x - np.log10(self.x_train_min)) / (np.log10(self.x_train_max) - np.log10(self.    x_train_min))\n",
    "        return log_x_norm\n",
    "    \n",
    "    def unscale_log_minmax(self, x_scaled):\n",
    "        x = x_scaled * (np.log10(self.x_train_max) - np.log10(self.x_train_min)) + np.log10(self.x_train_min)\n",
    "        return 10**x  \n",
    "        \n",
    "    def scale_error_log_minmax(self, err, x):\n",
    "        # need 1/np.log(10) factor bc working in base 10\n",
    "        dydx = 1./x * 1/np.log(10) * 1./(np.log10(self.x_train_max) - np.log10(self.x_train_min))\n",
    "        err_scaled = np.sqrt(np.multiply(dydx**2, err**2))\n",
    "        return err_scaled\n",
    "    \n",
    "    def scale_log(self, x):\n",
    "        return np.log10(x)\n",
    "    \n",
    "    def scale_log_error(self, err, x):\n",
    "        return (1./x) * (1/np.log(10)) * err\n",
    "    \n",
    "    def scale_minmax(self, x):\n",
    "        return (x - self.x_train_min) / (self.x_train_max - self.x_train_min)\n",
    "    \n",
    "    def unscale_minmax(self, x_scaled):\n",
    "        return x_scaled * (self.x_train_max - self.x_train_min) + self.x_train_min\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bdcc83",
   "metadata": {},
   "source": [
    "ok gaussian error is not the same here...! check how bacco is measuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a5fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Scaler()\n",
    "scaler.fit(Pk_train)\n",
    "Pk_train_scaled = scaler.scale(Pk_train)\n",
    "Pk_val_scaled = scaler.scale(Pk_val)\n",
    "Pk_test_scaled = scaler.scale(Pk_test)\n",
    "\n",
    "gaussian_error_pk_train_scaled = scaler.scale_error(gaussian_error_pk_train, Pk_train)\n",
    "gaussian_error_pk_val_scaled = scaler.scale_error(gaussian_error_pk_val, Pk_val)\n",
    "gaussian_error_pk_test_scaled = scaler.scale_error(gaussian_error_pk_test, Pk_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0ad849",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(Pk_train), np.max(Pk_train))\n",
    "print(np.min(Pk_train_scaled), np.max(Pk_train_scaled))\n",
    "\n",
    "print(np.min(Pk_test), np.max(Pk_test))\n",
    "print(np.min(Pk_test_scaled), np.max(Pk_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3623283",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Pk_train.shape, theta_train.shape, gaussian_error_pk_train.shape, n_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ca15d9",
   "metadata": {},
   "source": [
    "### Set up emulator, visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb6f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "emu_param_names = param_names\n",
    "#emu = baccoemu.Lbias_expansion(verbose=False)\n",
    "fn_emu = '/dipc_storage/cosmosims/data_share/lbias_emulator/lbias_emulator2.0.0'\n",
    "emu = baccoemu.Lbias_expansion(verbose=False, \n",
    "                               nonlinear_emu_path=fn_emu,\n",
    "                               nonlinear_emu_details='details.pickle',\n",
    "                               nonlinear_emu_field_name='NN_n',\n",
    "                               nonlinear_emu_read_rotation=False)\n",
    "#cosmo_params = setup_cosmo_emu()\n",
    "cosmo_params = genP.setup_cosmo_emu()\n",
    "# TODO save and read bias params\n",
    "bias_params = [1., 0., 0., 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9e96c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_bounds = {}\n",
    "# for pp, param_name in enumerate(param_names):\n",
    "#     dict_bounds[param_name] = [np.min(theta[:,pp]), np.max(theta[:,pp])]\n",
    "param_keys = emu.emulator['nonlinear']['keys']\n",
    "emu_bounds =  emu.emulator['nonlinear']['bounds']\n",
    "dict_bounds = {name: emu_bounds[param_keys.index(name)] for name in param_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aef9785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def setup_cosmo_emu():\n",
    "#     print(\"Setting up emulator cosmology\")\n",
    "#     cosmo_params = {\n",
    "#         #'omega_cold'    :  Om,\n",
    "#         #'sigma8_cold'   :  sigma8, # if A_s is not specified\n",
    "#         'omega_baryon'  :  param_dict_fixed['omega_baryon'],\n",
    "#         'ns'            :  param_dict_fixed['n_s'],\n",
    "#         #'hubble'        :  hubble,\n",
    "#         'neutrino_mass' :  0.0,\n",
    "#         'w0'            : -1.0,\n",
    "#         'wa'            :  0.0,\n",
    "#         'expfactor'     :  1\n",
    "#     }\n",
    "#     return cosmo_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_test = 130\n",
    "pk_data_unscaled = Pk_test_scaled[idx_test]\n",
    "pk_data = Pk_test_scaled[idx_test]\n",
    "\n",
    "err_1p = 0.01*pk_data_unscaled\n",
    "err_1p_scaled = scaler.scale_error(err_1p, pk_data_unscaled)\n",
    "err_gaussian_scaled = gaussian_error_pk_test_scaled[idx_test]\n",
    "var = err_gaussian_scaled**2 + err_1p_scaled**2\n",
    "cov_inv = np.diag(1/var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a48ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall quantities\n",
    "deltas_pk_per_err = []\n",
    "deltas_pk_per_err_scaled = []\n",
    "for i in range(Pk_train.shape[0]):\n",
    "    # if i % 100 == 0:\n",
    "    #     print(i)\n",
    "    # for pp in range(len(param_names)):\n",
    "    #     cosmo_params[emu_param_names[pp]] = theta[i][pp]\n",
    "    # _, pk_model_unscaled, _ = emu.get_galaxy_real_pk(bias=bias_params, k=k, \n",
    "    #                                                     **cosmo_params)\n",
    "    # in this case the orig emu pks are what we want\n",
    "    pk_model_unscaled = Pk_noiseless_train[i]\n",
    "    delta_pk_per_err = (pk_model_unscaled-Pk_train[i])/gaussian_error_pk_train[i]\n",
    "    deltas_pk_per_err.append(delta_pk_per_err)\n",
    "    \n",
    "    pk_model = scaler.scale(pk_model_unscaled)\n",
    "    delta_pk_per_err_scaled = (pk_model-Pk_train_scaled[i])/gaussian_error_pk_train_scaled[i]\n",
    "    deltas_pk_per_err_scaled.append(delta_pk_per_err_scaled)\n",
    "    \n",
    "deltas_pk_per_err = np.array(deltas_pk_per_err)\n",
    "delta_pk_per_err_16 = np.percentile(deltas_pk_per_err, 16, axis=0)\n",
    "delta_pk_per_err_84 = np.percentile(deltas_pk_per_err, 84, axis=0)\n",
    "delta_pk_per_err_std = np.std(deltas_pk_per_err, axis=0)\n",
    "\n",
    "deltas_pk_per_err_scaled = np.array(deltas_pk_per_err_scaled)\n",
    "deltas_pk_per_err_scaled_16 = np.percentile(deltas_pk_per_err_scaled, 16, axis=0)\n",
    "deltas_pk_per_err_scaled_84 = np.percentile(deltas_pk_per_err_scaled, 84, axis=0)\n",
    "deltas_pk_per_err_scaled_std = np.std(deltas_pk_per_err_scaled, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6202f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = 2, 1\n",
    "fig, axarr = plt.subplots(nrows, ncols, figsize=(6,6), sharex=True, height_ratios=[2,1])\n",
    "plt.subplots_adjust(hspace=0)\n",
    "    \n",
    "    \n",
    "# a few examples\n",
    "#colors = ['red', 'orange', 'green', 'blue', 'purple']\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "lw = 2\n",
    "show_error_all = True\n",
    "for i in range(6):\n",
    "    # for pp in range(len(param_names)):\n",
    "    #     cosmo_params[emu_param_names[pp]] = theta[i][pp]\n",
    "    # _, pk_model_unscaled, _ = emu.get_galaxy_real_pk(bias=bias_params, k=k, \n",
    "    #                                                     **cosmo_params)\n",
    "    # in this case the orig emu pks are what we want\n",
    "    pk_model_unscaled = Pk_noiseless_train[i]\n",
    "    \n",
    "    label_true, label_emu, label_stdev = None, None, None\n",
    "    if i==0:\n",
    "        label_true = 'training data (emulated)'\n",
    "        label_emu = r'emulated at true $\\theta$'\n",
    "        label_stdev = r'$0.2*\\sigma_\\text{stdev}(P_\\text{train}(k))$'\n",
    "    \n",
    "    if i==0 or show_error_all:\n",
    "        axarr[0].errorbar(k, Pk_train[i], yerr=gaussian_error_pk_train[i], \n",
    "                          ls='None', marker='o', markersize=3, alpha=0.5, label=label_true, color=colors[i])\n",
    "    else:\n",
    "        axarr[0].plot(k, Pk_train[i], ls='None', marker='o', markersize=3, alpha=0.5, label=label_true, color=colors[i])\n",
    "    \n",
    "    axarr[0].plot(k, pk_model_unscaled, ls='-', alpha=0.5, label=label_emu, color=colors[i], lw=lw)\n",
    "\n",
    "    axarr[1].plot(k, (pk_model_unscaled-Pk_train[i])/gaussian_error_pk_train[i], ls='-', alpha=0.5, color=colors[i], lw=lw)\n",
    "    axarr[1].axhline(0, color='grey', lw=0.5)\n",
    "    \n",
    "   \n",
    "\n",
    "#axarr[1].fill_between(k, delta_pk_per_err_16, delta_pk_per_err_84, color='cyan', alpha=0.3,\n",
    "#                      label='16-84 percentile of training set')\n",
    "axarr[1].fill_between(k, -delta_pk_per_err_std, delta_pk_per_err_std, color='salmon', alpha=0.3,\n",
    "                      label=r'$\\sigma$ of training set')\n",
    "\n",
    "# test data\n",
    "color_test = 'k'\n",
    "# for pp in range(len(param_names)):\n",
    "#     cosmo_params[emu_param_names[pp]] = theta_test[idx_test][pp]\n",
    "# _, pk_model_unscaled, _ = emu.get_galaxy_real_pk(bias=bias_params, k=k, \n",
    "#                                                  **cosmo_params)\n",
    "# in this case the orig emu pks are what we want\n",
    "pk_model_unscaled = Pk_noiseless_test[idx_test]\n",
    "axarr[0].plot(k, pk_model_unscaled, ls='-', alpha=0.5, color=color_test, lw=3)\n",
    "axarr[0].errorbar(k, Pk_test[idx_test], yerr=gaussian_error_pk_test[idx_test], \n",
    "                  ls='None', marker='o', markersize=6, alpha=0.5, label=f'test data', color=color_test)\n",
    "axarr[1].plot(k, (pk_model_unscaled-Pk_test[idx_test])/gaussian_error_pk_test[idx_test], ls='-', alpha=0.5, color=color_test)\n",
    "\n",
    " # 1p emu error\n",
    "err_1p = 0.01*pk_model_unscaled\n",
    "label_err1p = r'$0.01 \\times P_\\text{data}(k)$'\n",
    "axarr[1].fill_between(k, -err_1p/gaussian_error_pk_train[i], \n",
    "                          err_1p/gaussian_error_pk_train[i], color='grey', alpha=0.3,\n",
    "                            label=label_err1p)\n",
    "\n",
    "# plot settings\n",
    "plt.xscale('log')\n",
    "axarr[0].set_yscale('log')\n",
    "    \n",
    "plt.xlim(1e-2, 0.75)\n",
    "axarr[0].set_ylim(1e3, 3e5)\n",
    "axarr[1].set_ylim(-3, 3)\n",
    "    \n",
    "handles, labels = axarr[0].get_legend_handles_labels()\n",
    "order = [1,0,2]\n",
    "axarr[0].legend([handles[idx] for idx in order],[labels[idx] for idx in order], fontsize=12)\n",
    "# axarr[0].legend(fontsize=12)\n",
    "axarr[1].legend(fontsize=12)\n",
    "\n",
    "axarr[1].set_xlabel(r'$k \\,\\, [h \\,\\, {\\rm Mpc}^{-1}]$', fontsize=18)\n",
    "axarr[0].set_ylabel(r'$P(k) \\,\\, [h^{-3} \\,\\, {\\rm Mpc}^3]$', fontsize=18)    \n",
    "axarr[1].set_ylabel(r'$(P_\\text{emu}-P_\\text{data})/\\sigma_\\text{G}$', fontsize=18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5576ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = 2, 1\n",
    "fig, axarr = plt.subplots(nrows, ncols, figsize=(6,6), sharex=True, height_ratios=[2,1])\n",
    "plt.subplots_adjust(hspace=0)\n",
    "\n",
    "colors = ['red', 'orange', 'green', 'blue', 'purple']\n",
    "\n",
    "for i in range(5):\n",
    "    # for pp in range(len(param_names)):\n",
    "    #     cosmo_params[emu_param_names[pp]] = theta[i][pp]\n",
    "    # _, pk_model_unscaled, _ = emu.get_galaxy_real_pk(bias=bias_params, k=k, \n",
    "    #                                                     **cosmo_params)\n",
    "    # in this case the orig emu pks are what we want\n",
    "    pk_model_unscaled = Pk_noiseless_train[i]\n",
    "    pk_model_scaled = scaler.scale(pk_model_unscaled)\n",
    "    \n",
    "    label_true, label_emu, label_stdev = None, None, None\n",
    "    if i==0:\n",
    "        label_true = 'data (map2map)'\n",
    "        label_emu = 'emulated at true theta'\n",
    "        label_stdev = r'$0.2*\\sigma_\\text{stdev}(P_\\text{train}(k))$'\n",
    "    \n",
    "    if i==0:\n",
    "        axarr[0].errorbar(k, Pk_train_scaled[i], yerr=gaussian_error_pk_train_scaled[i], \n",
    "                          ls='--', marker='o', markersize=6, alpha=0.5, label=label_true, color=colors[i])\n",
    "    else:\n",
    "        axarr[0].plot(k, Pk_train_scaled[i], ls='--', marker='o', markersize=6, alpha=0.5, label=label_true, color=colors[i])\n",
    "    axarr[0].plot(k, pk_model_scaled, ls='-', alpha=0.5, label=label_emu, color=colors[i])\n",
    "    \n",
    "    axarr[1].plot(k, (pk_model_scaled-Pk_train_scaled[i])/gaussian_error_pk_train_scaled[i], ls='-', alpha=0.5, color=colors[i])\n",
    "    axarr[1].axhline(0, color='grey', lw=0.5)    \n",
    "\n",
    "axarr[1].fill_between(k, deltas_pk_per_err_scaled_16, deltas_pk_per_err_scaled_84, color='cyan', alpha=0.3,\n",
    "                      label='16-84 percentile of training set')\n",
    "\n",
    "# test data\n",
    "color_test = 'k'\n",
    "# for pp in range(len(param_names)):\n",
    "#     cosmo_params[emu_param_names[pp]] = theta_test[idx_test][pp]\n",
    "# _, pk_model_unscaled, _ = emu.get_galaxy_real_pk(bias=bias_params, k=k, \n",
    "#                                                  **cosmo_params)\n",
    "pk_model_unscaled = Pk_noiseless_test[idx_test]\n",
    "pk_model_scaled = scaler.scale(pk_model_unscaled)\n",
    "\n",
    "axarr[0].plot(k, pk_model_scaled, ls='-', alpha=0.5, color=color_test)\n",
    "axarr[0].errorbar(k, Pk_test_scaled[idx_test], yerr=gaussian_error_pk_test_scaled[idx_test], \n",
    "                  ls='--', marker='o', markersize=6, alpha=0.5, label=f'test data (idx_test={idx_test})', color=color_test)\n",
    "axarr[1].plot(k, (pk_model_scaled-Pk_test_scaled[idx_test])/gaussian_error_pk_test_scaled[idx_test], ls='-', alpha=0.5, color=color_test)\n",
    "\n",
    "# plot settings\n",
    "\n",
    "plt.xscale('log')\n",
    "#axarr[0].set_yscale('log')\n",
    "    \n",
    "axarr[1].set_ylim(-5, 5)\n",
    "    \n",
    "axarr[0].legend(fontsize=12)\n",
    "axarr[1].legend(fontsize=10)\n",
    "\n",
    "axarr[1].set_xlabel(r'$k \\,\\, [h \\,\\, {\\rm Mpc}^{-1}]$', fontsize=18)\n",
    "axarr[0].set_ylabel(r'$P(k) \\,\\, [h^{-3} \\,\\, {\\rm Mpc}^3]$', fontsize=18)    \n",
    "axarr[1].set_ylabel(r'$(P_\\text{emu}-P_\\text{data})/\\sigma_\\text{G}$', fontsize=18)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfea6d75",
   "metadata": {},
   "source": [
    "### Set up and train Moment Network model\n",
    "\n",
    "Following demos at https://github.com/NiallJeffrey/MomentNetworks/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20776f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout, ReLU, LeakyReLU, Input\n",
    "\n",
    "class neural_net():\n",
    "    \"\"\"\n",
    "    A simple MLP with LeakyReLU activation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, output_size, \n",
    "                 hidden_size=32, learning_rate=None,\n",
    "                 activation='leakyrelu',\n",
    "                 alpha=0.1):\n",
    "        \"\"\"\n",
    "        Initialisation\n",
    "        :param map_size: size of square image (there are map_size**2 pixels)\n",
    "        :param learning_rate: learning rate for the optimizer\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        if activation=='leakyrelu':\n",
    "            self.activation_func = LeakyReLU\n",
    "            self.activation_kwargs = {'alpha': alpha}\n",
    "        elif activation=='relu':\n",
    "            self.activation_func = ReLU\n",
    "            self.activation_kwargs = {}\n",
    "        else:\n",
    "            raise ValueError(f\"Activation function {activation} not recognized\")\n",
    "\n",
    "        \n",
    "    def model(self):\n",
    "        print(self.input_size)\n",
    "        \n",
    "        input_data = (Input(shape=(self.input_size,)))\n",
    "\n",
    "        x1 = Dense(self.hidden_size, input_dim=self.input_size, kernel_initializer='normal')(input_data)\n",
    "        x1 = self.activation_func(**self.activation_kwargs)(x1)\n",
    "        x2 = Dense(self.hidden_size, kernel_initializer='normal')(x1)\n",
    "        x2 = self.activation_func(**self.activation_kwargs)(x2)\n",
    "        x3 = Dense(self.hidden_size, kernel_initializer='normal')(x2)\n",
    "        x3 = self.activation_func(**self.activation_kwargs)(x3)\n",
    "        #x4 = Dense(self.output_size, kernel_initializer='normal', activation='relu')(x3)        \n",
    "        x4 = Dense(self.output_size, kernel_initializer='normal')(x3)        \n",
    "\n",
    "        dense_model = Model(input_data, x4)\n",
    "        dense_model.summary()\n",
    "\n",
    "        if self.learning_rate is None:\n",
    "            dense_model.compile(optimizer='adam', loss='mse')\n",
    "        else:\n",
    "            dense_model.compile(optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate), loss='mse')\n",
    "\n",
    "        return dense_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbc50c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_moment:\n",
    "    model_instance = neural_net(n_dim, n_params, \n",
    "                                hidden_size=64,\n",
    "                                learning_rate=1e-3) \n",
    "    regression = model_instance.model() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc5d5ca",
   "metadata": {},
   "source": [
    "Train initial model (basic MLP), as usual, on labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f07db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(Pk_train_scaled), np.min(Pk_val_scaled), np.min(theta_train), np.min(theta_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef95bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_moment:\n",
    "    callback = tf.keras.callbacks.EarlyStopping(patience=25,\n",
    "                                restore_best_weights=True,\n",
    "                                start_from_epoch=200)\n",
    "    history = regression.fit(Pk_train_scaled, theta_train,\n",
    "                            epochs=2500, batch_size=64, shuffle=True,\n",
    "                            callbacks=[callback],\n",
    "                            validation_data=(Pk_val_scaled, theta_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb46ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_moment:\n",
    "    plt.plot(history.history['loss'], color='blue', label='training set', alpha=0.6)\n",
    "    plt.plot(history.history['val_loss'], color='limegreen', label='validation set', alpha=0.6)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    plt.yscale('log')\n",
    "    #plt.ylim(0, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef5ffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_moment:\n",
    "    theta_train_pred = regression.predict(np.atleast_2d(Pk_train_scaled))\n",
    "    theta_val_pred = regression.predict(np.atleast_2d(Pk_val_scaled))\n",
    "    theta_test_pred = regression.predict(np.atleast_2d(Pk_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e17db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.isnan(theta_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bf2718",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_moment:\n",
    "    for pp in range(n_params):\n",
    "        plt.figure(figsize=(3,3))\n",
    "        #delta_param = (theta_train_pred[:,pp] - theta_train[:,pp])/theta_train[:,pp]\n",
    "        delta_param = (theta_test_pred[:,pp] - theta_test[:,pp])/theta_test[:,pp]\n",
    "        plt.hist(delta_param, bins=20, alpha=0.5, color='salmon')\n",
    "        plt.xlabel(rf'$\\Delta${param_labels[pp]}/{param_labels[pp]}')\n",
    "        plt.ylabel(r'$N$ in bin')\n",
    "        plt.axvline(0, color='grey')\n",
    "        plt.xlim(-np.max(abs(delta_param)), np.max(abs(delta_param)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85da5cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982f7f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum((theta_test_pred - theta_test)/theta_test, axis=1).shape)\n",
    "idxs_test_ordered = np.argsort(np.sum(np.abs((theta_test_pred - theta_test)/theta_test), axis=1))\n",
    "print(np.where(idxs_test_ordered==idx_test)[0])\n",
    "idx_test_good = idxs_test_ordered[199]\n",
    "print(idx_test, theta_test[idx_test], theta_test_pred[idx_test])\n",
    "print(idx_test_good, theta_test[idx_test_good], theta_test_pred[idx_test_good])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a74c3c",
   "metadata": {},
   "source": [
    "Get means and residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8e3674",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_covariances = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6377e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_moment:\n",
    "    cov_dict = {}\n",
    "\n",
    "    covariances_train = []\n",
    "    covariances_val = []\n",
    "    covariances_test = []\n",
    "    count = 0\n",
    "    for i in range(n_params):\n",
    "        for j in range(n_params):\n",
    "            if not include_covariances:\n",
    "                if i!=j:\n",
    "                    continue\n",
    "                \n",
    "            if j<i:\n",
    "                cov_dict[(i,j)] = cov_dict[(j,i)]\n",
    "                continue\n",
    "                \n",
    "            covariances_train.append((theta_train[:,i]-theta_train_pred[:,i])* \\\n",
    "                            (theta_train[:,j]-theta_train_pred[:,j]))\n",
    "                        \n",
    "            covariances_val.append((theta_val[:,i]-theta_val_pred[:,i])* \\\n",
    "                                (theta_val[:,j]-theta_val_pred[:,j]))\n",
    "            \n",
    "            covariances_test.append((theta_test[:,i]-theta_test_pred[:,i])* \\\n",
    "                                 (theta_test[:,j]-theta_test_pred[:,j]))\n",
    "            \n",
    "            cov_dict[(i,j)] = count\n",
    "            count += 1\n",
    "            \n",
    "    covariances_train = np.array(covariances_train).T\n",
    "    covariances_val = np.array(covariances_val).T\n",
    "    covariances_test = np.array(covariances_test).T\n",
    "\n",
    "    n_covs = covariances_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04314fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pp in range(n_covs):\n",
    "    plt.figure(figsize=(3,2))\n",
    "    plt.hist(covariances_train[:,pp], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84384957",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_cov = Scaler(func='minmax')\n",
    "scaler_cov.fit(covariances_train)\n",
    "covariances_train_scaled = scaler_cov.scale(covariances_train)\n",
    "covariances_val_scaled = scaler_cov.scale(covariances_val)\n",
    "covariances_test_scaled = scaler_cov.scale(covariances_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989810ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(covariances_train, axis=0), np.max(covariances_train, axis=0))\n",
    "print(np.min(covariances_train_scaled, axis=0), np.max(covariances_train_scaled, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a895a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pp in range(n_covs):\n",
    "    plt.figure(figsize=(3,2))\n",
    "    plt.hist(covariances_train_scaled[:,pp], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d013cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not include_covariances:\n",
    "    assert np.all(covariances_train>=0)==True, \"Should all be positive!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cov_predmean(theta, theta_pred):\n",
    "    cov = np.empty((theta.shape[1], theta.shape[1]))\n",
    "    n_samples = theta.shape[0]\n",
    "    for pp_i in range(theta.shape[1]):\n",
    "        for pp_j in range(theta.shape[1]):\n",
    "            val = 0\n",
    "            for i in range(n_samples):\n",
    "                val += (theta[i][pp_i] - theta_pred[i][pp_i])*(theta[i][pp_j] - theta_pred[i][pp_j])\n",
    "            cov[pp_i,pp_j] = val/n_samples\n",
    "    return cov\n",
    "\n",
    "def compute_cov_truemean(theta, bias=False):\n",
    "    cov = np.empty((theta.shape[1], theta.shape[1]))\n",
    "    theta_means = np.mean(theta, axis=0)\n",
    "    n_samples = theta.shape[0]\n",
    "    if bias:\n",
    "        norm = n_samples\n",
    "    else:\n",
    "        norm = n_samples - 1\n",
    "    for pp_i in range(theta.shape[1]):\n",
    "        for pp_j in range(theta.shape[1]):\n",
    "            val = 0\n",
    "            for i in range(n_samples):\n",
    "                val += (theta[i][pp_i] - theta_means[pp_i])*(theta[i][pp_j] - theta_means[pp_j])\n",
    "            cov[pp_i,pp_j] = val/norm\n",
    "    return cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acc2759",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_moment:\n",
    "    model_instance = neural_net(n_dim, n_covs, \n",
    "                                hidden_size=64,\n",
    "                                learning_rate=1e-2,\n",
    "                                activation='leakyrelu',\n",
    "                                alpha=0.1)\n",
    "    regression_var_unknown_mean = model_instance.model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d632e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_moment:\n",
    "    \n",
    "    callback = tf.keras.callbacks.EarlyStopping(patience=20,\n",
    "                                restore_best_weights=True,\n",
    "                                start_from_epoch=100)\n",
    "    \n",
    "    history_var = regression_var_unknown_mean.fit(Pk_train_scaled,\n",
    "                                            covariances_train_scaled,\n",
    "                                            epochs=500, batch_size=64, shuffle=True,\n",
    "                                            callbacks=[callback],\n",
    "                                            validation_data = (Pk_val_scaled,\n",
    "                                                                covariances_val_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec95f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_moment:\n",
    "    plt.plot(history_var.history['loss'], color='blue', label='training set', alpha=0.6)\n",
    "    plt.plot(history_var.history['val_loss'], color='limegreen', label='validation set', alpha=0.6)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    plt.yscale('log')\n",
    "    #plt.ylim(0, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0271261",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_var_obs_test_scaled = (regression_var_unknown_mean.predict(np.atleast_2d(pk_data))[0])\n",
    "predicted_var_obs_test = scaler_cov.unscale(predicted_var_obs_test_scaled)\n",
    "predicted_var_obs_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3914a786",
   "metadata": {},
   "source": [
    "### SBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbdf9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sbi:\n",
    "    # TODO deal with validation fraction being taken from training set\n",
    "    inference, posterior = sbi_tools.train_model(\n",
    "        theta_train,\n",
    "        Pk_train_scaled,\n",
    "        prior=sbi_tools.get_prior(dict_bounds),\n",
    "        training_batch_size=32,\n",
    "        learning_rate=1e-3,\n",
    "        validation_fraction=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1b92ec",
   "metadata": {},
   "source": [
    "### Explicit likelihood setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a9a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for emcee\n",
    "def log_prior(theta):\n",
    "    for pp in range(len(param_names)):\n",
    "       if (theta[pp] < dict_bounds[param_names[pp]][0]) or (theta[pp] >= dict_bounds[param_names[pp]][1]):\n",
    "           return -np.inf\n",
    "    return 0.0\n",
    "\n",
    "# for dynesty\n",
    "def prior_transform(u):\n",
    "\n",
    "    u_transformed = []\n",
    "    for pp in range(len(param_names)):\n",
    "        width = dict_bounds[param_names[pp]][1] - dict_bounds[param_names[pp]][0]\n",
    "        min_bound = dict_bounds[param_names[pp]][0]\n",
    "        \n",
    "        u_t = width*u[pp] + min_bound\n",
    "        u_transformed.append(u_t)           \n",
    "\n",
    "    return np.array(u_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f509714",
   "metadata": {},
   "outputs": [],
   "source": [
    "global pk_data, cov_inv\n",
    "\n",
    "def log_likelihood(theta):\n",
    "    for pp in range(len(param_names)):\n",
    "        cosmo_params[emu_param_names[pp]] = theta[pp]\n",
    "    _, pk_model_unscaled, _ = emu.get_galaxy_real_pk(bias=bias_params, k=k, \n",
    "                                                **cosmo_params)\n",
    "    pk_model = scaler.scale(pk_model_unscaled)\n",
    "    diff = pk_data-pk_model\n",
    "    # print(theta)\n",
    "    # print(cosmo_params)\n",
    "    # print(pk_data)\n",
    "    # print(pk_model)\n",
    "    # print(cov_inv[0,0], cov_inv[1,1], cov_inv[2,2])\n",
    "    # print(-0.5*np.dot(diff,np.dot(cov_inv,diff)))\n",
    "    \n",
    "    # print()\n",
    "    return -0.5*np.dot(diff,np.dot(cov_inv,diff))\n",
    "\n",
    "def log_posterior(theta):\n",
    "    lp = log_prior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a0069b",
   "metadata": {},
   "source": [
    "### Test on a model from the test set (held-out data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ff292",
   "metadata": {},
   "source": [
    "Moment network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93525508",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_moment:\n",
    "\n",
    "    predicted_mean_obs_test = regression.predict(np.atleast_2d(pk_data))\n",
    "    print(predicted_mean_obs_test)\n",
    "\n",
    "    predicted_var_obs_test_scaled = (regression_var_unknown_mean.predict(np.atleast_2d(pk_data))[0])\n",
    "    predicted_var_obs_test = scaler_cov.unscale(predicted_var_obs_test_scaled)\n",
    "\n",
    "    moment_network_param_cov_test = np.zeros((n_params, n_params))\n",
    "    for i in range(n_params):\n",
    "        for j in range(n_params):\n",
    "            if not include_covariances:\n",
    "                if i!=j:\n",
    "                    continue\n",
    "            moment_network_param_cov_test[i,j] = predicted_var_obs_test[cov_dict[(i,j)]]\n",
    "    print(moment_network_param_cov_test)\n",
    "\n",
    "    moment_network_samples_test = np.array(np.random.multivariate_normal(predicted_mean_obs_test[0],\n",
    "                                        moment_network_param_cov_test,int(1e6)),dtype=np.float32)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0008ef10",
   "metadata": {},
   "source": [
    "SBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ed7827",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_sbi:\n",
    "    samples_sbi_test = sbi_tools.sample_posteriors_theta_test(\n",
    "        posterior,\n",
    "        np.atleast_2d(pk_data),\n",
    "        dict_bounds,\n",
    "        N_samples=10000\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea85723",
   "metadata": {},
   "source": [
    "MCMC, Dynesty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd2cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_dynesty:\n",
    "    with dynesty.pool.Pool(n_threads, log_likelihood, prior_transform) as pool:\n",
    "        sampler_test = dynesty.NestedSampler(pool.loglike, pool.prior_transform, n_params, \n",
    "                                            nlive=20, bound='single')\n",
    "        sampler_test.run_nested(dlogz=0.01)\n",
    "        \n",
    "    results_test = sampler_test.results\n",
    "    samples_dynesty_test = results_test.samples_equal()\n",
    "    print(samples_dynesty_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d823bc2",
   "metadata": {},
   "source": [
    "MCMC, emcee:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637cee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_emcee:\n",
    "    \n",
    "    n_burn = 100\n",
    "    n_steps = 500 # 50000\n",
    "    n_walkers = 4 * n_params\n",
    "\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    theta_0 = np.array([[rng.uniform(low=dict_bounds[param_name][0],high=dict_bounds[param_name][1]) \n",
    "                        for param_name in param_names] for _ in range(n_walkers)])\n",
    "\n",
    "    start = time.time()\n",
    "    if n_threads>1:\n",
    "        with Pool(processes=n_threads) as pool:\n",
    "            sampler_emcee = emcee.EnsembleSampler(n_walkers, n_params, log_posterior, pool=pool)\n",
    "            _ = sampler_emcee.run_mcmc(theta_0, n_steps, progress=True) \n",
    "    else:\n",
    "        sampler_emcee = emcee.EnsembleSampler(n_walkers, n_params, log_posterior)\n",
    "        _ = sampler_emcee.run_mcmc(theta_0, n_steps, progress=True) \n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Time: {end-start} s ({(end-start)/60} min)\")\n",
    "\n",
    "    samples_emcee = sampler_emcee.get_chain(discard=n_burn, flat=True, thin=1)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db707a5d",
   "metadata": {},
   "source": [
    "### Plot contours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a57877",
   "metadata": {},
   "source": [
    "Is there a \"true\" covariance in this toy case i can compare to?!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c7cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(extents={}, plot_moment=True, plot_sbi=True, plot_emcee=True, plot_dynesty=True):\n",
    "\n",
    "    c = chainconsumer.ChainConsumer()\n",
    "\n",
    "    if run_moment and plot_moment:\n",
    "        c.add_chain(chainconsumer.Chain(\n",
    "                    samples=pd.DataFrame(moment_network_samples_test, columns=param_names),\n",
    "                    name='Moment Network', color='blue')\n",
    "                    )\n",
    "\n",
    "    if run_sbi and plot_sbi:\n",
    "        c.add_chain(chainconsumer.Chain(\n",
    "                    samples=pd.DataFrame(samples_sbi_test, columns=param_names),\n",
    "                    name='SBI', color='orange',\n",
    "                    smooth=1, bins=10)\n",
    "                    )\n",
    "\n",
    "    if run_emcee and plot_emcee:\n",
    "        c.add_chain(chainconsumer.Chain(\n",
    "                    samples=pd.DataFrame(samples_emcee, columns=param_names),\n",
    "                    name='MCMC (emcee)', color='purple', ls='--',\n",
    "                    smooth=2, bins=10)\n",
    "                    )\n",
    "\n",
    "    if run_dynesty and plot_dynesty:\n",
    "        c.add_chain(chainconsumer.Chain(\n",
    "                    samples=pd.DataFrame(samples_dynesty_test, columns=param_names),\n",
    "                    name='MCMC (Dynesty)', color='green', \n",
    "                    smooth=2, bins=5)\n",
    "                    )\n",
    "\n",
    "    c.set_plot_config(\n",
    "        chainconsumer.PlotConfig(\n",
    "            flip=True,\n",
    "            labels=param_label_dict,\n",
    "            contour_label_font_size=12,\n",
    "            extents=extents,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    truth_loc = dict(zip(param_names, theta_test[idx_test]))\n",
    "    c.add_truth(chainconsumer.Truth(location=truth_loc))\n",
    "\n",
    "    fig = c.plotter.plot(figsize = (5,4) )\n",
    "    if save_plots:\n",
    "        plt.savefig(f'{plot_dir}/contours_test{idx_test}{tag_emuPk}{tag_fit}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20d7b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contours()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cb2818",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contours(plot_moment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45ace89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contours(plot_emcee=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb0bd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contours(extents=dict_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a054cde5",
   "metadata": {},
   "source": [
    "### For all test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b83d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas_from_truth = []\n",
    "sigmas_total = []\n",
    "\n",
    "theta_test_pred = regression.predict(Pk_test_scaled)\n",
    "vars_test_pred_scaled = regression_var_unknown_mean.predict(Pk_test_scaled)\n",
    "vars_test_pred = scaler_cov.unscale(vars_test_pred_scaled)\n",
    "#print(theta_pred_test.shape, vars_pred_test.shape)\n",
    "\n",
    "for i in range(Pk_test_scaled.shape[0]):\n",
    "    pk_data_test = Pk_test_scaled[i]\n",
    "    \n",
    "    moment_network_param_cov_test = np.zeros((n_params, n_params))\n",
    "    for ii in range(n_params):\n",
    "        for jj in range(n_params):\n",
    "            if not include_covariances:\n",
    "                if ii!=jj:\n",
    "                    continue\n",
    "            moment_network_param_cov_test[ii,jj] = vars_test_pred[i][cov_dict[(ii,jj)]]\n",
    "\n",
    "    err = np.sqrt(np.diag(moment_network_param_cov_test))\n",
    "    sigma_from_truth = (theta_test_pred[i] - theta_test[i])/err\n",
    "    sigmas_from_truth.append(sigma_from_truth)\n",
    "    \n",
    "    diff = theta_test_pred[i] - theta_test[i]\n",
    "    cov_pred_inv = np.linalg.inv(moment_network_param_cov_test)\n",
    "    sigma_total = diff.T @ cov_pred_inv @ diff\n",
    "    sigmas_total.append(sigma_total)\n",
    "\n",
    "sigmas_from_truth = np.array(sigmas_from_truth)\n",
    "sigmas_total = np.array(sigmas_total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b58a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum((theta_test_pred - theta_test)/theta_test, axis=1).shape)\n",
    "idxs_test_orderedchi2 = np.argsort(sigmas_total)\n",
    "print(np.where(idxs_test_orderedchi2==idx_test)[0])\n",
    "idx_test_goodchi2 = idxs_test_orderedchi2[201]\n",
    "print(idx_test, sigmas_total[idx_test], theta_test[idx_test], theta_test_pred[idx_test])\n",
    "print(idx_test_goodchi2, sigmas_total[idx_test_goodchi2], theta_test[idx_test_goodchi2], theta_test_pred[idx_test_goodchi2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d61a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_nan = (vars_test_pred[:,0] < 0) | (vars_test_pred[:,2] < 0)\n",
    "# print(np.sum(idx_nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e22e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_normal = np.linspace(-3, 3, 30)\n",
    "mean, variance = 0, 1\n",
    "y_normal = np.exp(-np.square(x_normal-mean)/2*variance)/(np.sqrt(2*np.pi*variance))\n",
    "#xmin, xmax = -np.max(abs(sigmas_from_truth[:,pp])[~idx_nan]), np.max(abs(sigmas_from_truth[:,pp])[~idx_nan])\n",
    "xmin, xmax = -3, 3\n",
    "for pp in range(n_params):\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.title(rf'{param_labels[pp]}')\n",
    "    plt.hist(sigmas_from_truth[:,pp], bins=np.linspace(xmin, xmax, 20),\n",
    "             color='salmon', alpha=0.5, density=True)\n",
    "    plt.xlabel(r'$\\sigma_\\text{MN}$')\n",
    "    plt.ylabel(r'$N$ in bin')\n",
    "\n",
    "    plt.axvline(0, color='grey')\n",
    "    \n",
    "    plt.xlim(xmin, xmax)\n",
    "    plt.plot(x_normal, y_normal, color='black', lw=1, label=r'$\\mathcal{N}(0,1)$')\n",
    "    plt.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e7105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "x_normal = np.linspace(0, 10, 30)\n",
    "mean, variance = 0, 1\n",
    "y_normal = np.exp(-np.square(x_normal-mean)/2*variance)/(np.sqrt(2*np.pi*variance))\n",
    "#xmin, xmax = -np.max(abs(sigmas_from_truth[:,pp])[~idx_nan]), np.max(abs(sigmas_from_truth[:,pp])[~idx_nan])\n",
    "xmin, xmax = 0, 8\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.xlabel(r'$\\chi^2$')\n",
    "plt.ylabel(r'$N$ in bin')\n",
    "plt.hist(sigmas_total, bins=np.linspace(xmin, xmax, 20), color='salmon', alpha=0.5, density=True)\n",
    "df = 2\n",
    "plt.plot(x_normal, chi2.pdf(x_normal, df), color='black', lw=1, label=rf'$\\chi^2$ PDF, df={df}')\n",
    "\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.legend(fontsize=12)\n",
    "#plt.plot(x_normal, y_normal, color='black', lw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2207fd71",
   "metadata": {},
   "source": [
    "### Try another test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f0fcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_test = 8\n",
    "# pk_data = Pk_test_scaled[idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38eca61",
   "metadata": {},
   "source": [
    "Moment network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d36dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run_moment:\n",
    "#     predicted_mean_obs_test = regression.predict(np.atleast_2d(pk_data))\n",
    "#     predicted_var_obs_test_scaled = (regression_var_unknown_mean.predict(np.atleast_2d(pk_data))[0])\n",
    "#     predicted_var_obs_test = scaler_cov.unscale(predicted_var_obs_test_scaled)\n",
    "\n",
    "#     moment_network_param_cov_test = np.zeros((n_params, n_params))\n",
    "\n",
    "#     for i in range(n_params):\n",
    "#         for j in range(n_params):\n",
    "#             if not include_covariances:\n",
    "#                 if i!=j:\n",
    "#                     continue\n",
    "#             moment_network_param_cov_test[i,j] = predicted_var_obs_test[cov_dict[(i,j)]]\n",
    "#     print(moment_network_param_cov_test)\n",
    "    \n",
    "#     moment_network_samples_test = np.array(np.random.multivariate_normal(predicted_mean_obs_test[0],\n",
    "#                                   moment_network_param_cov_test,int(1e6)),dtype=np.float32)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf25582",
   "metadata": {},
   "source": [
    "MCMC, Dynesty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb34779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run_dynesty:\n",
    "#     with dynesty.pool.Pool(n_threads, log_likelihood, prior_transform) as pool:\n",
    "#         sampler_test = dynesty.NestedSampler(pool.loglike, pool.prior_transform, n_params, \n",
    "#                                             nlive=20, bound='single')\n",
    "#         sampler_test.run_nested(dlogz=0.01)\n",
    "        \n",
    "#     results_test = sampler_test.results\n",
    "#     samples_dynesty_test = results_test.samples_equal()\n",
    "#     print(samples_dynesty_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d20f4bf",
   "metadata": {},
   "source": [
    "MCMC, emcee:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9d0751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run_emcee:\n",
    "#     rng = np.random.default_rng(seed=42)\n",
    "#     theta_0 = np.array([[rng.uniform(low=dict_bounds[param_name][0],high=dict_bounds[param_name][1]) \n",
    "#                         for param_name in param_names] for _ in range(n_walkers)])\n",
    "\n",
    "#     start = time.time()\n",
    "#     if n_threads>1:\n",
    "#         with Pool(processes=n_threads) as pool:\n",
    "#             sampler_emcee = emcee.EnsembleSampler(n_walkers, n_params, log_posterior, pool=pool)\n",
    "#             _ = sampler_emcee.run_mcmc(theta_0, n_steps, progress=True) \n",
    "#     else:\n",
    "#         sampler_emcee = emcee.EnsembleSampler(n_walkers, n_params, log_posterior)\n",
    "#         _ = sampler_emcee.run_mcmc(theta_0, n_steps, progress=True) \n",
    "#     end = time.time()\n",
    "\n",
    "#     print(f\"Time: {end-start} s ({(end-start)/60} min)\")\n",
    "\n",
    "#     samples_emcee = sampler_emcee.get_chain(discard=n_burn, flat=True, thin=1)\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8294828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_contours()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223651c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bemuenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
